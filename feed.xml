<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://fooyuhui.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fooyuhui.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-20T02:20:53+00:00</updated><id>https://fooyuhui.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Summary of Loco-manipulation Papers</title><link href="https://fooyuhui.github.io/blog/2025/locomani/" rel="alternate" type="text/html" title="Summary of Loco-manipulation Papers"/><published>2025-02-15T00:00:00+00:00</published><updated>2025-02-15T00:00:00+00:00</updated><id>https://fooyuhui.github.io/blog/2025/locomani</id><content type="html" xml:base="https://fooyuhui.github.io/blog/2025/locomani/"><![CDATA[<h3 id="umi-on-legs"><a href="https://arxiv.org/abs/2407.10353">UMI on Legs</a></h3> <ol> <li>scaling task-centric data in the real world without a robot.</li> <li>scaling robot-centric data in simulation without task simulations. (training policies to track end-effector trajectories instead of interacting with objects)</li> <li>using end-effector trajectories in the task-frame as the interface</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/umi-on-legs-480.webp 480w,/assets/img/blog/umi-on-legs-800.webp 800w,/assets/img/blog/umi-on-legs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/umi-on-legs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Two main components</p> <ol> <li>a high-level diffusion-based manipulation policy (input wrist-mounted camera views and output sequences of end-effector pose targets)</li> <li>low-level whole-body controller (tracking end-effecotr pose targets)</li> </ol> <h3 id="vbc"><a href="https://arxiv.org/abs/2403.16967">VBC</a></h3> <ul> <li>a low-level control policy using all degrees of freedom to track the body velocities along with the end-effector position</li> <li>a high-level task-planning policy proposing the velocities and end-effector position based on visual inputs</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vbc-480.webp 480w,/assets/img/blog/vbc-800.webp 800w,/assets/img/blog/vbc-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/vbc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="omnigrasp"><a href="https://arxiv.org/abs/2407.11385">OmniGrasp</a></h3> <p>Using a pretrained universal dexterous motion representation as the action space.</p> <h3 id="catchit"><a href="https://arxiv.org/abs/2409.10319">CatchIt!</a></h3> <p>A two-stage RL Framework. The first stage is training the control policies for the base and arm in the tracking task. The second stage is training the hand’s control policy while fine-tuning the base and arm’s policy from the tracking task.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/catchit-480.webp 480w,/assets/img/blog/catchit-800.webp 800w,/assets/img/blog/catchit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/catchit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="pedipulate"><a href="https://arxiv.org/abs/2402.10837">Pedipulate</a></h3> <p>Using the legs of a legged robot for manipulation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/pedipulate-480.webp 480w,/assets/img/blog/pedipulate-800.webp 800w,/assets/img/blog/pedipulate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/pedipulate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>train reinforcement learning-based pedipulation controller by training a policy that tracks foot position targets.</li> <li>Curriculum learning: adaptive command sampling from easy to hard tasks.</li> </ul> <h3 id="learning-visual-quadrupedal-loco-manipulation-from-demonstrations"><a href="https://arxiv.org/abs/2403.20328">Learning Visual Quadrupedal Loco-Manipulation from Demonstrations</a></h3> <ul> <li>BC at the high-level manipulation planning and RL at the low-level joint position control.</li> <li>parameterize the manipulation trajectory of the endeffector for better integration of RL and BC. (Bezier curve control points)</li> </ul> <p>pipeline:</p> <ol> <li>train RL-based trajectory tracking controller</li> <li>collect data for planner (input: visual point cloud, robot state, output: end-effector trajectory)</li> <li>train BC-based planner</li> <li>deploy the system</li> </ol> <h3 id="guided-reinforcement-learning-for-robust-multi-contact-loco-manipulation"><a href="https://arxiv.org/abs/2410.13817">Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</a></h3> <p>Use dynamically feasible demonstrations generated from a TO-based framework to guide the RL agent in learning complex behaviors.</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="robotics"/><summary type="html"><![CDATA[some notes on loco-manipulation papers, including legged robots, mobile manipulators, and so on.]]></summary></entry><entry><title type="html">ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills</title><link href="https://fooyuhui.github.io/blog/2025/asap/" rel="alternate" type="text/html" title="ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://fooyuhui.github.io/blog/2025/asap</id><content type="html" xml:base="https://fooyuhui.github.io/blog/2025/asap/"><![CDATA[<blockquote> <p>Takeaway: We can train a delta (residual) action model to compensate for the dynamics mismatch.</p> </blockquote> <p>You can find the paper <a href="https://arxiv.org/abs/2502.0114">here</a>.</p> <h2 id="method">Method</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/asap-480.webp 480w,/assets/img/blog/asap-800.webp 800w,/assets/img/blog/asap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/asap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Two-stage training process:</p> <p>Pre-traing: learning agile humanoid skills in simulation.</p> <ul> <li>Retargeting human video data.</li> <li>Phase-based motion tracking policy training.</li> </ul> <p>Fine-tuning: aligning simulation and real-world physics.</p> <ul> <li>Train a delta (residual) action model to compensate for the dynamics mismatch using roll-out data from the pre-trained policy.</li> <li>Freeze the delta action model and fine-tune the pre-trained policy using RL.</li> <li>Deploy the fine-tuned policy on the real robot.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The idea of using a delta action model and real-world data to align simulation with real-world physics is intriguing.</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="robotics"/><summary type="html"><![CDATA[ASAP Reading Notes]]></summary></entry><entry><title type="html">DEXTERITYGEN Foundation Controller for Unprecedented Dexterity</title><link href="https://fooyuhui.github.io/blog/2025/dex_gen/" rel="alternate" type="text/html" title="DEXTERITYGEN Foundation Controller for Unprecedented Dexterity"/><published>2025-02-10T00:00:00+00:00</published><updated>2025-02-10T00:00:00+00:00</updated><id>https://fooyuhui.github.io/blog/2025/dex_gen</id><content type="html" xml:base="https://fooyuhui.github.io/blog/2025/dex_gen/"><![CDATA[<blockquote> <p>Takeaway: DexGen is a generative model that can translate an unsafe, coarse motion command produced by external policy to safe and fine actions.</p> </blockquote> <p>You can find the paper <a href="https://arxiv.org/abs/2502.04307">here</a>.</p> <h2 id="insight">Insight</h2> <p>To achieve effective teleoperation for a dexterous hand, merely retargeting hand motion is insufficient. We can use RL to pretrain large-scale dexterous motion primitives, and then roll out some data using this policy to train a dexterous foundational controller.</p> <h2 id="method">Method</h2> <h5 id="large-scale-behavior-dataset-generation">Large-Scale Behavior Dataset Generation.</h5> <ol> <li>Generate a set of object grasps using Grasp Analysis and Rapidly-exploring Random Tree (RRT). Each generated grasp is defined as a tuple (hand joint position, object pose). (over 100K grasp)</li> <li>Train a anygrasp-to-anygrasp policy using RL. We set the goal to be a randomly selected nearby grasp using k nearest-neighbor search. (The authors also introduce other tasks besides anygrasp-to-anygrasp, such fine-grained manipulation and free finger moving etc.)</li> <li>Use this anygrasp-to-anygrasp policy to rollout grasp transition sequences to cover all the possible hand-object interation modes. ($1 \times 10^{10}$ transitions)</li> </ol> <h5 id="dexgen-model">DexGen Model</h5> <p>Two modules: a diffusion model and a inverse dynamics model.</p> <ol> <li>Diffusion Model: given robot state and a mode conditioning variable, characterize the distribution of robot finger keypoint motions. (3D keypoint motions $\Delta x \in \mathbb{R}^{T \times K \times 3}$, $T$ is the future horizon, $K$ is the number of keypoints)</li> <li>Inverse Dynamics Model: given the current observation and the future keypoints, predict the executable actions.</li> </ol> <p>The whole system takes robot state, external motion conditioning, and mode conditioning as input.</p> <p>The motion conditioning is not fed into the diffusion model directly but as the gradient guidance during the diffusion sampling.</p> <h5 id="inference-motion-conditioning-with-guided-sampling">Inference: Motion Conditioning with Guided Sampling</h5> <p>The goal is to sample a keypoint motion that is both safe and maximally preserve the input reference motion.</p> <p>We can use the following formula to sample the keypoint motion:</p> \[\Delta x \sim p_\theta(\Delta x \mid o)\mathrm{exp}(-\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}))\] <p>Here, $\Delta x_{\text{input}}$ is the input commanded fingertip offset, and $\mathrm{Dist}$ is a distance metric.</p> <p>Since the action of the robot hand has a high degree of freedom, naive sampling strategies become computationally intractable. The authors propose using gradient guidance in the diffusion sampling process to incorporate motion conditioning. In each diffusion step, we adjust the denoised sample $\Delta x$ by subtracting $\alpha\sum\nabla_{\Delta x}\text{Dist}(\Delta x, \Delta x_\text{input})$ as a guide. Here $\alpha$ is a parameter of the strength of the guidance to be tuned.</p> <h2 id="conclusion">Conclusion</h2> <p>The authors demonstrate that DexGen can effectively translate unsafe, coarse motion commands to safe and fine actions. The model can be used in a teleoperation setting to improve the dexterity of the robot hand.</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="robotics"/><summary type="html"><![CDATA[DexGen Reading Notes]]></summary></entry></feed>