<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://fooyuhui.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fooyuhui.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-11T06:44:23+00:00</updated><id>https://fooyuhui.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills</title><link href="https://fooyuhui.github.io/blog/2025/asap/" rel="alternate" type="text/html" title="ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://fooyuhui.github.io/blog/2025/asap</id><content type="html" xml:base="https://fooyuhui.github.io/blog/2025/asap/"><![CDATA[<blockquote> <p>Takeaway: We can train a delta (residual) action model to compensate for the dynamics mismatch.</p> </blockquote> <p>You can find the paper <a href="https://arxiv.org/abs/2502.0114">here</a>.</p> <h2 id="method">Method</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/asap-480.webp 480w,/assets/img/blog/asap-800.webp 800w,/assets/img/blog/asap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/asap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Two-stage training process:</p> <p>Pre-traing: learning agile humanoid skills in simulation.</p> <ul> <li>Retargeting human video data.</li> <li>Phase-based motion tracking policy training.</li> </ul> <p>Fine-tuning: aligning simulation and real-world physics.</p> <ul> <li>Train a delta (residual) action model to compensate for the dynamics mismatch using roll-out data from the pre-trained policy.</li> <li>Freeze the delta action model and fine-tune the pre-trained policy using RL.</li> <li>Deploy the fine-tuned policy on the real robot.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The idea of using a delta action model and real-world data to align simulation with real-world physics is intriguing.</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="robotics"/><summary type="html"><![CDATA[ASAP Reading Notes]]></summary></entry><entry><title type="html">DEXTERITYGEN Foundation Controller for Unprecedented Dexterity</title><link href="https://fooyuhui.github.io/blog/2025/dex_gen/" rel="alternate" type="text/html" title="DEXTERITYGEN Foundation Controller for Unprecedented Dexterity"/><published>2025-02-10T00:00:00+00:00</published><updated>2025-02-10T00:00:00+00:00</updated><id>https://fooyuhui.github.io/blog/2025/dex_gen</id><content type="html" xml:base="https://fooyuhui.github.io/blog/2025/dex_gen/"><![CDATA[<blockquote> <p>Takeaway: DexGen is a generative model that can translate an unsafe, coarse motion command produced by external policy to safe and fine actions.</p> </blockquote> <p>You can find the paper <a href="https://arxiv.org/abs/2502.04307">here</a>.</p> <h2 id="insight">Insight</h2> <p>To achieve effective teleoperation for a dexterous hand, merely retargeting hand motion is insufficient. We can use RL to pretrain large-scale dexterous motion primitives, and then roll out some data using this policy to train a dexterous foundational controller.</p> <h2 id="method">Method</h2> <h5 id="large-scale-behavior-dataset-generation">Large-Scale Behavior Dataset Generation.</h5> <ol> <li>Generate a set of object grasps using Grasp Analysis and Rapidly-exploring Random Tree (RRT). Each generated grasp is defined as a tuple (hand joint position, object pose). (over 100K grasp)</li> <li>Train a anygrasp-to-anygrasp policy using RL. We set the goal to be a randomly selected nearby grasp using k nearest-neighbor search. (The authors also introduce other tasks besides anygrasp-to-anygrasp, such fine-grained manipulation and free finger moving etc.)</li> <li>Use this anygrasp-to-anygrasp policy to rollout grasp transition sequences to cover all the possible hand-object interation modes. ($1 \times 10^{10}$ transitions)</li> </ol> <h5 id="dexgen-model">DexGen Model</h5> <p>Two modules: a diffusion model and a inverse dynamics model.</p> <ol> <li>Diffusion Model: given robot state and a mode conditioning variable, characterize the distribution of robot finger keypoint motions. (3D keypoint motions $\Delta x \in \mathbb{R}^{T \times K \times 3}$, $T$ is the future horizon, $K$ is the number of keypoints)</li> <li>Inverse Dynamics Model: given the current observation and the future keypoints, predict the executable actions.</li> </ol> <p>The whole system takes robot state, external motion conditioning, and mode conditioning as input.</p> <p>The motion conditioning is not fed into the diffusion model directly but as the gradient guidance during the diffusion sampling.</p> <h5 id="inference-motion-conditioning-with-guided-sampling">Inference: Motion Conditioning with Guided Sampling</h5> <p>The goal is to sample a keypoint motion that is both safe and maximally preserve the input reference motion.</p> <p>We can use the following formula to sample the keypoint motion:</p> \[\Delta x \sim p_\theta(\Delta x \mid o)\mathrm{exp}(-\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}))\] <p>Here, $\Delta x_{\text{input}}$ is the input commanded fingertip offset, and $\mathrm{Dist}$ is a distance metric.</p> <p>Since the action of the robot hand has a high degree of freedom, naive sampling strategies become computationally intractable. The authors propose using gradient guidance in the diffusion sampling process to incorporate motion conditioning. In each diffusion step, we adjust the denoised sample $\Delta x$ by subtracting $\alpha\sum\nabla_{\Delta x}\text{Dist}(\Delta x, \Delta x_\text{input})$ as a guide. Here $\alpha$ is a parameter of the strength of the guidance to be tuned.</p> <h2 id="conclusion">Conclusion</h2> <p>The authors demonstrate that DexGen can effectively translate unsafe, coarse motion commands to safe and fine actions. The model can be used in a teleoperation setting to improve the dexterity of the robot hand.</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="robotics"/><summary type="html"><![CDATA[DexGen Reading Notes]]></summary></entry></feed>